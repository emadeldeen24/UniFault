# UniFault Foundation Model 

This repository provides a PyTorch-based training pipeline for fine-tuning 
the UniFault Foundation model on time-series data.

---

## ğŸ“ Directory Structure

```
.
â”œâ”€â”€ fine_tune.py         # Main fine-tuning script
â”œâ”€â”€ model/
â”‚   â””â”€â”€ model.py                # Main Model (SSL-training and fine-tuning) implementation
â”‚   â””â”€â”€ Transformer_utils.py    # Transformer model implementation
â”œâ”€â”€ datalaoders/
â”‚   â””â”€â”€ pretraining_dataloader.py   # Data loading during SSL-based pretraining
â”‚   â””â”€â”€ train_dataloader.py         # Data loading in supervised or fine-tuning
â”œâ”€â”€ utils.py                # Helper functions
â”œâ”€â”€ checkpoints/            # Output directory for checkpoints and metrics
â”œâ”€â”€ lightning_logs/         # Directory to store pretrained model checkpoints
```

---

## ğŸš€ Quick Start

### ğŸ§ª Supervised Training From Scratch (Start from randomly initialized weights)

```bash
python fine_tune.py --load_from_pretrained False
```

### â™»ï¸ Fine-Tune a Pretrained Model

```bash
python fine_tune.py \
  --load_from_pretrained True \
  --pretraining_epoch_id <epoch_id>
```
The pretrained_model_dir is expected to be pretrained_models/MODLE_TYPE


---

## âš™ï¸ Command-Line Arguments
### General Args
| Argument                 | Description                                                  | Default   |
| ------------------------ |--------------------------------------------------------------|-----------|
| `--data_path`            | Path to dataset                                              | -         |
| `--data_id`              | Identifier for the dataset variant                           | Ex: `IMS` |
| `--data_percentage`      | Fraction of data used (e.g., `1` = 1% and `100` = full data) | `1`       |
| `--gpu_id`               | GPU device ID                                                | `0`       |


### Model Args
| Model Argument | Description                                                  | Note                                  |
|----------------|--------------------------------------------------------------|---------------------------------------|
| `--model_id`   | Identifier for model variant (used in naming logs)           | Change each run                       |
| `--embed_dim`  | Embedding dimension for Transformer                          | Change for (Tiny-Small-Base) variants |
| `--heads`      | Number of attention heads                                    | Change for (Tiny-Small-Base) variants |
| `--depth`      | Number of transformer blocks                                 | Change for (Tiny-Small-Base) variants |
| `--patch_size` | Patch size for ViT input                                     | KEEP FIXED                            |
| `--dropout`    | Dropout rate                                                 | KEEP FIXED                                 |

### Loading from a pretrained model
| Argument                 | Description                                                                                                                                |
| ------------------------ |--------------------------------------------------------------------------------------------------------------------------------------------|
| `--load_from_pretrained` | Boolean flag to use pretrained weights                                                                                                     |
| `--pretrained_model_dir` | Directory containing pretrained checkpoint (No need to add checkpoint name -- Just its directory)                                          |
| `--pretraining_epoch_id` | Epoch number of pretrained checkpoint (because we save all the pretraining epochs, so this one is to select which one do you want to load) |

| Argument                 | Description                                                  | Default  |
| ------------------------ |--------------------------------------------------------------| -------- |
| `--num_epochs`           | Number of training epochs                                    | `10`     |
| `--batch_size`           | Batch size                                                   | `64`     |
| `--lr`                   | Learning rate                                                | `3e-4`   |
| `--wt_decay`             | Weight decay                                                 | `1e-4`   |
| `--random_seed`          | Seed for reproducibility                                     | `42`     |

---

## ğŸ“ Outputs

After training, the following files are generated in `checkpoints/<run_name>/`:

* `confusion_matrix.png` â€“ Visualization of model predictions
* `classification_report.txt` â€“ Detailed precision/recall/F1 report
* `loss.png`, `accuracy.png` â€“ Training and validation curves
* `best.ckpt` â€“ Best checkpoint based on validation F1 score

---

## ğŸ§  How Pretrained Loading Works

If `--load_from_pretrained` is set:

* A checkpoint is loaded from `lightning_logs/<pretrained_model_dir>/pretrain-epoch=<id>.ckpt`
* Only layers with matching names and sizes are loaded (via filtered `state_dict`)

---

## ğŸ§ª Example:

Train from scratch:

```bash
python fine_tune.py \
  --data_path "/path/to/your/data" \
  --data_id "YourDatasetID" \
  --model_id "scratch_run" \
  --load_from_pretrained False \
  --num_epochs 20
```

Fine-tune a model:

```bash
python fine_tune.py \
  --load_from_pretrained True \
  --pretraining_epoch_id 1 \
  --model_id "finetune_run"
```